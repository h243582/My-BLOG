# 一、概述

集合分为两类，`Collection`和`Map`

Collection实现了`Iterable`接口，同时又分为`List`和`Set`

List分为`ArrayList、LinkedList、Vector`

Set分为`HashSet、TreeSet、LinkedHashSet`

Map分为`HashMap、HashTable、TreeMap、properties、LinkedHashMap`

# 二、**Iterator**

  foreach就是简化版的Iterator

  遍历集合三种方法：**迭代器，增强for，普通for**

# 三、Collection

## **List**

添加取出有序，有索引，能存放重复，能存放NULL，可以存放多个

### **`ArrayList`**

1. 线程不安全(没有synchronized)，Transient（不会被序列化)
2. 单线程情况下效率比Vector高
3. 数组实现
4. 初始化的时候若是无参则一开始大小为0，第一次扩容为10，以后一直1.5倍扩容；
5. 若是有参，则一直1.5倍扩容

### **Vector**

线程安全->带有synchronized

数组实现

初始化无参默认10，扩容2倍

有参扩容2倍

### **LinkedList**

线程不安全,没有实现同步

实现了双向链表和双端队列的特点

双向链表，两个分别指向头尾节点的指针

添加删除元素相对来说效率较高

**==改查选ArrayList，增删选LinkedList==**

## **Set**

添加取出无序，无索引，不能存放重复，能存放NULL，但是只能存放一个

虽然取出的顺序不是添加的顺序，但是是固定的

因为没有索引，所以不支持普通for循环的方式遍历数据

### **HashSet**

实际上是HashMap

不保证元素有序，取决于hash后，在确定索引的结果

区分对象是否重复是先hash值，再equals

底层是HashMap，而HashMap的底层是数据+链表+红黑树->先是Hash表，当hash表的单一链表长度超过 8 个，并且数组的大小超过64的时候，那个链表结构就会转为红黑树结构

hashcode得到的值不是hash值，因为还做了额外的处理

第一次添加的时候，table数组扩容到16，临界值是12(四分之三)

临界值：虽然大小没有达到上限，但是要做一个缓冲，所以大小到了临界值会扩容

每一次table大小扩容是原来的两倍，临界值更新为四分之三

四分之三是加载因子，0.75

### **LinkedHashSet**

底层是LinkedHashMap，维护了一个数组+双向链表

根据元素的hashcode值来决定元素的存储位置，同时使用链表来维护元素的次序(跨链表)，使得插入和遍历顺序一样

存在head和tail指针

不允许添加重复元素

### **TreeSet**

排序

无参构造器默认不排序

初始化的时候在构造器中传入自己的比较器

底层就是TreeMap

​        

# 四、**Map**

key值不允许重复（hash表），key和value都可以为null，但是key为null的只能存在一个，key和value是一对一关系

有相同的key时就是替换value

K-V是为了方便程序员的遍历，其实会创建一个EntrySet集合，该集合存放的元素类型是Entry，而一个Entry对象就有

k，v，EntrySet>,即：transient Set> entrySet

entrySet中，定义的类型是Map.Entry， 但是实际上存放的还是HashMap$Node

这是因为HashMap$Node implements Map.Entry

之所以将HashMap$Node放到entrySet，是因为后者提供了很重要的方法，方便程序员来遍历

比如说getKey()、getValue()

entry集合并没有复制数据，只有单纯的引用

遍历的六种方式：Map.keySet()得到所有的key->Set集合，Map.values()得到所有的value->collection集合,

转换成Map.Entry的方式->Set集合,而每一种又分为增强for和迭代器的方式遍历，故有六种

## **HashMap**

没有实现线程同步，因此不安全

与HashSet一样，不保证映射的顺序，因为底层是以数组+链表+红黑树的方式存储的

源码

1.执行构造器new HashMap()

初始化加载因子为0.75

HashMap$Node[] table = null

2.执行put调用hash算法

return (key == null) ? 0 : (h = key.hashcode()) ^ (h >>> 16)

public V put(K key,V value){

​      return putVal(hash(key),value,false,true);

​    }

3.执行putVal

​    if(table == null || table.length == 0)

​      调用resize()扩容,第一次为16

​      添加节点

​      若有相同的则直接break

​      大小是否到达临界值

​      是则扩容

​       

### HashMap为什么线程不安全

​		put的时候造成数据覆盖，get的时候造成死循环

​		1、put的时候导致的多线程数据不一致。

​			有两个线程A和B，首先A希望插入一个key-value对到HashMap中，首先计算记录所要落到的桶的索引坐标，然后获取到该桶里面的链表头结点，此时线程A的时间片用完了，而此时线程B被调度得以执行，和线程A一样执行，只不过线程B成功将记录插到了桶里面，假设线程A插入的记录计算出来的桶索引和线程B要插入的记录计算出来的桶索引是一样的，那么当线程B成功插入之后，线程A再次被调度运行时，它依然持有过期的链表头但是它对此一无所知，以至于它认为它应该这样做，如此一来就覆盖了线程B插入的记录，这样线程B插入的记录就凭空消失了，造成了数据不一致的行为。

​		2、get操作可能因为resize而引起死循环（cpu100%）

​			有两个线程同时需要执行resize操作，我们原来的桶数量为2，记录数为3，需要resize桶到4，原来的记录分别为：[1,A],[2,B],[3,C]，

在原来的map里面，我们发现这三个entry都落到了新的第二个桶里面假设线程thread1执行到了transfer方法的Entry next = e.next这一句，然后时间片用完了，此时的e = [1,A], next = [2,B]。线程thread2被调度执行并且顺利完成了resize操作，需要注意的是，此时的[2,B]的next为[1,A]。此时线程thread1重新被调度运行，此时的thread1持有的引用是已经被thread2 resize之后的结果。

线程thread1首先将[1,A]迁移到新的数组上，然后再处理[2,B]，而[2,B]被链接到了[1,A]的后面，处理完[2,B]之后，就需要处理[2,B]的next了啊，而通过thread2的resize之后，[2,B]的next变为了[1,A]，此时，[1,A]和[2,B]形成了环形链表，在get的时候，如果get的key的桶索引和[1,A]和[2,B]一样，那么就会陷入死循环。

注

新建的数组一开始每一个线程都有1个，为局部数组

​				

扩容

初始化大小

​	    JDK7 -> 找到一个大于值  2的幂次方 >= Size

​	modCount   

​	    修改次数 

​	    快速失败的机制

​	    一个线程在遍历，另一个在修改，HashMap很可能出现问题

​	Put

头插法：每次都将新的节点当作头节点，Next指向当前的头节点，然后数组的Node指针指向这个新节点，看起来像是整个链表向下移算出新节点的位置的时候是用了Hash%数组长度（Hash值太大，所以要对数组取余）的方式找到该节点该存放的位置得到Hash的时候是用了HashCode方法，然后在此基础上又做了一些操作

扩容条件

大小到达临界值 && 插入的位置不为空，即桶里面有数据（jdk1.8则没有第二部分判断）

新链表位置

要么和旧位置一样，要么旧的位置加上新扩容的大小（5+16就到了21）

迁移链表

创建一个新的局部数组，大小为两倍

For循环->每一个链表都要迁移

第二个循环->链表的每一个节点->  e != null  ->  这个循环完成了本链表迁移，头插法的方式，链表顺序反过来了，第一个在最底下

e指向第一个节点

next = e.next

e.next = newTable[i]

newTable[i] = e

e = next	

**JDK1.7的ConcurrentHashMap**

JDK1.7的HashMap基础上,key不能为NULL

​	Put

Synchronized,HashMap的Put上锁锁的是整Table,ConcurrentHashMap锁的是一个segment

​	分段锁，锁的是segment

​	结构

​	【[][][]】【[][][]】【[][][]】

​	【】-> segment   [] -> HashEntryTable[]

​	初始化

​		初始大小16(所有的segment里面的HashEntry[]的数量加起来的总和)，加载因子0.75，最大并发数16(segment的个数)初始化的时候每一个segment最少存2个HashEntryTable[],里面存储的HashEntryTable[]数量必须是2的幂次方，且都一样

指定HashTableEntry = 33,segment = 16,33 / 16 = 2 .. 1,则有16个segment，每一个里面存放4个HashEntryTable[]

​		初始化segment0

​	扩容

​		只扩容当前Segment下的HashEntryTable[] -> 2倍

​		初始化的时候HashEntryTable[].length都一致，后面就不一致了

​		Segment大小不会变

​	与HashMap的区别

​		新节点的位置

​		两个遍历

​			第一个遍历  计算每个节点将要迁移到新链表的位置，并且做一些记录，只记录相同位置节点、且在旧链表连续的第一个节点，这样子移动的时候只需要移动这个第一个节点，其余的连续的节点都会跟随这个节点，思想类似于蜘蛛纸牌

第二个遍历头插法

​		Put

​			put的时候，若当前Segment为NULL，则先需要New一个segment对象(属性直接Copy->segment0的属性,就不需要重新计算)，再调用它的Put方法

​			首先HashCode & segment[].length - 1 		确定segment的位置

​			然后HashCode & HashEntryTable[].length - 1  确定数组下标的位置

​	一个线程Put的时候想要在链表上加锁

​		首先tryLock()【Lock回阻塞，TryLock()会返回True或者false，不阻塞】，假如加锁失败，就先遍历一下这个链表，看看有没有重复的key，遍历完以后发现还没有加到锁就New一个新的节点，结束后若还没有加到锁就再一次重试tryLock()，同时重试次数+1，当重试的数量达到一定限制就直接调用Lock()阻塞，偶数次重试的时候去看看链表有没有发生变化，若发生了变化就再去遍历一下

​	锁

​		CAS锁 -> UNSAFE

**1.8的ConcurrentHashMap**

1.7有Segment，1.8没有，1.8和HashMap一样是数组+链表+红黑树的结构

1.8没有分段锁，通过synchronized加锁

第一个线程Put的时候发现算出来的Hash值 = MOVED(-1)，这就代表与此同时还有别的线程在进行扩容，然后的第一个线程会帮助HashMap扩容，有两个线程同时扩容优点就是扩容速度会快点(正常情况Hash值不是-1)，Hash值不是 MOVED 的时候，会判断要插入的地方是红黑树还是链表，对链表里面的第一个节点加锁，

加完锁以后看看加锁的节点还是不是第一个节点，不是的话就去找第一个节点加锁，这第一个节点是

链表

遍历链表，然后看看链表大小有没有到一定的数量，看看要不要树化

红黑树

HaspMap是TreeNode，而ConcurrentHashMap是TreeBin，一开始一样改成双向链表，Hd指向第一个头节点，遍历链表生成红黑树，生成一个TreeBin对象，对象里面就是一棵红黑树,将TreeBin放入Table[index],TreeBin代表整个红黑树的根节点

与HashMap的TreeNode区别

​	HashMap多线程put的时候一开始对头节点，然后put以后可能红黑树会旋转，这样子头节点就变了，不符合逻辑，这时候又来了一个线程，可以加锁，但是第一个锁没有释放。

​	TreeBin的话只会锁一个TreeBin对象,理解为套了一个壳,TreeBin就是红黑树，里面有一个属性Root，指向红黑树的根节点

扩容

​	fork/join大体的执行过程

​	将一个大的任务拆分成若干个独立的小任务，然后用多线程并行处理这些小任务，处理完得到结果后再进行合并(join)就得到我们

​								

------------------------部分没学--------------------------------------	

​			

**JDK1.7和1.8  HashMap的区别**

1.7是头插法，1.8是尾插法

​	链表本身长度>=8,并且数组长度>=64的时候，变成红黑树，不算新节点本身

两个版本的扩容都是为了减短链表

​	生成红黑树的过程

​		根据Node生成一个双向链表TreeNode，以第一个节点为根节点，遍历每一个节点，插入到红黑树里面去，红黑树的根节点变成双向链表的第一个节点，放入数组中

​	1.8的扩容

​		链表

​		与1.7扩容一样，这个链表的每一个节点的新位置，要么和旧链表一致，要么在新数组的index + old.table.length

​		迁移链表				

与1.7的ConcurrentHashMap差不多，只不过后者要求旧链表连续才放一起，1.8HashMap直接遍历整个链表，将所有在新数组一致位置的节点连起来，1.7 HashMap是一个一个节点算，再移,1.8 HashMap则是先算所有的，再移。

直接将每个链表的第一个节点放在数组中(并非1.7的头插法)

​		红黑树

​		将node链表转成treenode双向链表，生成红黑树，将红黑树头节点对应的treenode节点移到链表头部

​		迁移红黑树

​		先看看能不能将红黑树拆成两个链表

​			拆分过程

​			根据链表的计算位置的算法拆成高位链表和低位链表，也就是数组对应的两个位置，因为迁移只会出现两个位置

​			低位链表判断过程

​			节点数 ≤6 的话，treenode低位链表转换成node链表，放入table【index】，否则将低位链表的头节点放入对应位置，然后判断高位链表是否存在，存在的话就对低位链表树化，（高位链表不存在就说明低位链表就是一颗完整的红黑树，移动头节点就相当于移动整棵树，存在就代表红黑树拆分过，所以需要树化）

​			高位链表和低位链表一样的判断逻辑

**HashTable**

存放的Key和Value都不能为NULL，否则抛出异常

 HashMap线程不安全，而HashTable线程安全

  底层有数组HashTable$Entry[] ,初始化大小为11

  临界值为11*0.75 = 8

  扩容机制: 原来的大小 * 2 + 1

**properties**

  继承HashTable实现了Map接口

  与HashTable一样Key和Value都不能NULL

  相同Key替换Value

**TreeMap**

  实现参考TreeSet

底层红黑树

**Collections工具类的了解**

Arraylist list = new ArrayList();

  Collections.reverse(list);

**开发中如何选择**

先判断存储类型(一组对象、一组键值对)

  一组对象

   允许重复

改查多：ArrayList，数组

增删多：LinkedList，双向链表

安全：Vector

不允许重复

无序：

HashSet     

HashMap->Hash表->数组+链表+红黑树

​    插入和取出一致：

LinkedHashSet  

数组+双向链表

底层就是LinkedHashMap,LinkedHashMap底层还是HashMap

​    排序：TreeSet

   一组键值对

​    键无序：

   HashMap     

   Hash表 jdk7->数组+链表    

   jdk8->数组+链表+红黑树

​    键有序：TreeMap

​    键插入和取出一致：LinkedHashMap

​    读取文件：properties

